import numpy as np
import open3d as o3d
from typing import Tuple, Optional

def depth_to_camera_frame_point_cloud(
          depth: np.ndarray,
          K: np.ndarray,
          rgb: Optional[np.ndarray] = None,
  ) -> Tuple[np.ndarray, Optional[np.ndarray]]:

      """the goal of this script is to take the rgbd inputs that are generated by the extrinsics/intrinsics of the camera
      and make them into 3d point clouds
      [u]   [fx  0 cx]   [x]
      [v] = [ 0 fy cy] * [y]
      [1]   [ 0  0  1]   [z]

      where the u, v are in pixel space and the xyz are 3d space so this is the inverse function from the projection
      x = (u - cx)*z / fx
      y = (v - cy)*z / fy 
      z = depth

      inputs are depth = (H, W)
      K = camera intrinsics matrix
      rgb = optional rgb image of shape (H, W, 3) in [0, 1]

      returns points of shape (N, 3) where N = H * W
      colors = optional array of colors
      """

      H, W = depth.shape

      # create pixel coordinate grids (u is horizontal and v is vertical)
      u, v = np.meshgrid(np.arange(W), np.arange(H)) 

      # extract the camera intrinsics
      fx, fy = K[0, 0], K[1, 1]
      cx, cy = K[0, 2], K[1, 2]

      # inverse projection for each pixel at once
      z = depth
      x = (u - cx) * z / fx
      y = (v - cy) * z / fy

      # now x, y, z are all (H, W) arrays
      # stack them vertically and reshape to (H*W, 3)
      points = np.stack([x, y, z], axis=-1)  # shape is (H, W, 3)

      # np.stack() creates a new dimension to add on
      # hence the points is (h, w, 3) since it stacked 3 arrays
      points = points.reshape(-1, 3)  # shape is now (H*W, 3)
      # flatten for a list of points so that it can be indexed as point[k] = [x, y, z] for the kth point

      colors = None
      if rgb is not None:
          colors = rgb.reshape(-1, 3)  # (H*W, 3) to match points

      return points, colors


def transform_points_to_world(
          points: np.ndarray,
          world_T_camera: np.ndarray
  ) -> np.ndarray:
      """This is a function that takes in the points in the camera frame and then projects them onto the world
      frame using the extrinsics matrix

      world_T_camera = [
      [r11, r12, r13, tx],  # Rotation + translation in X
      [r21, r22, r23, ty],  # Rotation + translation in Y
      [r31, r32, r33, tz],  # Rotation + translation in Z
      [ 0,   0,   0,   1]   # Homogeneous row ]
      where:
      - Top-left 3Ã—3 block: Rotation matrix (R)
      - Right column (first 3 rows): Translation vector (t)
      """

      N = points.shape[0]
      camera_T_world = np.linalg.inv(world_T_camera)

      # numpy's inv() knows how to handle the special cases of transformation matrices
      # next convert to homogeneous coordinates

      ones = np.ones((N, 1))
      # hstack() stacks arrays horizontally so across columns

      points_hom = np.hstack([points, ones]) 
      # each of the points now has a 1 at the end of it being in homogeneous coordinates

      points_hom_T = points_hom.T  
      # each point is now a column
      points_hom_world_T = camera_T_world @ points_hom_T

      # switching back to the row format
      points_hom_world = points_hom_world_T.T

      # extract the 3D coordinates aka bring it back from being homogeneous
      points_world = points_hom_world[:, :3]

      return points_world

def create_point_cloud_from_rgbd(
          rgb: np.ndarray,
          depth: np.ndarray,
          K: np.ndarray,
          world_T_camera: np.ndarray,
          filter_invalid: bool = True,
          bbox: Optional[np.ndarray] = None
  ) -> o3d.geometry.PointCloud:
     """The goal of this function is to use the above functions and create the point clouds 
     """

     points_cam, colors = depth_to_camera_frame_point_cloud(depth, K, rgb)
     points = transform_points_to_world(points_cam, world_T_camera)

     for point in points:
          
     pcd = None
     return pcd 